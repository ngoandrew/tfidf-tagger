{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "STOP = set(stopwords.words('english'))\n",
    "textCorpus = []\n",
    "numberOfDocs = len(data)\n",
    "\n",
    "stemToTag = dict()\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "  text = re.sub('[^a-zA-Z]', ' ', row['Text'])\n",
    "  title = re.sub('[^a-zA-Z]', ' ', row['Title'])\n",
    "  text = text.lower().split()\n",
    "  title = title.lower().split()\n",
    "\n",
    "  textArr = []\n",
    "  for w in text:\n",
    "    if not w in STOP and len(w) > 1:\n",
    "      textArr.append(ps.stem(w))\n",
    "      if ps.stem(w) not in stemToTag:\n",
    "        stemToTag[ps.stem(w)] = w\n",
    "\n",
    "  titleArr = []\n",
    "  for w in title:\n",
    "    if not w in STOP and len(w) > 1:\n",
    "      titleArr.append(ps.stem(w))\n",
    "      if ps.stem(w) not in stemToTag:\n",
    "        stemToTag[ps.stem(w)] = w\n",
    "\n",
    "  text = ' '.join(textArr)\n",
    "  title = ' '.join(titleArr)\n",
    "\n",
    "  processedText = title + ' ' + text\n",
    "  textCorpus.append(processedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_calculator(corpusIndex):\n",
    "    vectorizer = CountVectorizer(max_df = 0.8, max_features = 2000)    \n",
    "    word_count_vector = vectorizer.fit_transform(textCorpus)\n",
    "\n",
    "    transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    transformer.fit(word_count_vector)\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    stemmedArticle = textCorpus[corpusIndex]\n",
    "     \n",
    "    tf_idf_vector = transformer.transform(vectorizer.transform([stemmedArticle]))\n",
    "    return feature_names, tf_idf_vector, stemmedArticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_tags(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    results = {}\n",
    "    for idx, score in sorted_items:\n",
    "      results[feature_names[idx]] = score\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article not in Database\n"
     ]
    }
   ],
   "source": [
    "articleToTag = input(\"Enter the name of the Article to Tag: \")\n",
    "row = data.loc[data['Title'] == articleToTag]\n",
    "if row.empty:\n",
    "  print(\"Article not in Database\")\n",
    "else:\n",
    "  articleIndex = data.index[data['Title'] == articleToTag].tolist()[0]\n",
    "  feature_names, tf_idf_vector, stemmedArticle = tfidf_calculator(articleIndex)\n",
    "\n",
    "  coordinate_vector = tf_idf_vector.tocoo()\n",
    "  sorted_items = sorted(zip(coordinate_vector.col, coordinate_vector.data), \\\n",
    "                      key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "  keywords = top_n_tags(feature_names,sorted_items,20)\n",
    "\n",
    "  top_tags = []\n",
    "  for tag in keywords.keys():\n",
    "    top_tags.append(tag)\n",
    "\n",
    "  tags = [stemToTag[s].capitalize() for s in top_tags]\n",
    "  print(f\"Tags for the Article: {articleToTag}\")\n",
    "  print(tags)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e458013d4848352eab0fe2932e94a547ae8f30a0495a6594abf4ea07415e147"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
